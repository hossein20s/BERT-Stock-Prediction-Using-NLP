{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTlib.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hossein20s/BERT-Stock-Prediction-Using-NLP/blob/master/BERTlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tz7MElzPtxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VERSION = 'Allen1.0' #@param {type:\"string\"}\n",
        "\n",
        "print('BERTlib Version ' + VERSION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPFyKYJ0LC9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following should be defined in the caller pythin file\n",
        "#DATA_DIR\n",
        "#TASK\n",
        "#START_FROM_PRETRAINED\n",
        "\n",
        "import os\n",
        "\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "# NUM_TRAIN_EPOCHS = 3.0\n",
        "MAX_SEQ_LENGTH = 128 # or 400\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500 # or 1000\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "\n",
        "ESTIMATOR_DIR = DATA_DIR + '/data4estimator/'\n",
        "os.makedirs(ESTIMATOR_DIR, exist_ok=True)\n",
        "RESULT_DIR = DATA_DIR + '/results/'\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "INPUT_DATA_DIR = DATA_DIR + '/inputData/'\n",
        "os.makedirs(INPUT_DATA_DIR, exist_ok=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRQozsrgOKgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
        "\n",
        "#BERT_PRETRAINED_DIR = 'gs://medicalblockchain_dev/bert-checkpoints/models/SciBert'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL \n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLRlcWTzMBEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from google.colab import auth\n",
        "import pprint\n",
        "import json\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "print(tf.__version__)\n",
        "\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7RNFIJ2K7J0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "SRC_DIR='/content/gdrive/My Drive/src/'\n",
        "REPO_DIR=SRC_DIR +  '/bert_repo/'\n",
        "if not REPO_DIR in sys.path:\n",
        "  sys.path.append(REPO_DIR)\n",
        "  \n",
        "###Delete all flags before declare#####\n",
        "\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "\n",
        "import run_classifier\n",
        "#import importlib\n",
        "#importlib.reload(run_classifier)\n",
        "#print('reloading .....')\n",
        "\n",
        "\n",
        "processors = {\n",
        "  \"cola\": run_classifier.ColaProcessor,\n",
        "  \"mnli\": run_classifier.MnliProcessor,\n",
        "  \"mrpc\": run_classifier.MrpcProcessor,\n",
        "}\n",
        "processor = processors[TASK.lower()]()\n",
        "label_list = processor.get_labels()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FkKJ18jLpkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OUTPUT_DIR = OUTPUT_DIR_TFHUB.replace('bert-tfhub', 'bert-checkpoints')\n",
        "\n",
        "\n",
        "if START_FROM_PRETRAINED:\n",
        "  INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "else:\n",
        "  INIT_CHECKPOINT=MODEL_INPUT_DIR\n",
        "\n",
        "tf.gfile.MakeDirs(MODEL_OUTPUT_DIR)\n",
        "\n",
        "import modeling\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "import tokenization\n",
        "import run_classifier_with_tfhub\n",
        "#tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True) # causing problem\n",
        "train_examples = processor.get_train_examples(ESTIMATOR_DIR)\n",
        "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)  # We'll set sequences to be at most 128 tokens long.\n",
        "\n",
        "print('loading model from checkpoint directoryy {}', INIT_CHECKPOINT)\n",
        "\n",
        "#import run_classifier_with_tfhub\n",
        "#model_fn = run_classifier_with_tfhub.model_fn_builder(\n",
        "#  bert_hub_module_handle=BERT_MODEL_HUB\n",
        "import run_classifier\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "#import run_pretraining\n",
        "#model_fn = run_pretraining.model_fn_builder( # not sure what is this class for\n",
        "  bert_config=bert_config,\n",
        "  init_checkpoint=INIT_CHECKPOINT,\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  use_one_hot_embeddings=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6nzcePTLRnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import run_classifier\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import time\n",
        "\n",
        "# Train the model\n",
        "def model_train(estimator):\n",
        "  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
        "  # Compute number of train and warmup steps from batch size\n",
        "  \n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  start = time.time()\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  #clear_output()\n",
        "  end = time.time()\n",
        "  print('  Num examples = {}'.format(len(train_examples)))\n",
        "  print('  Batch size = {} and TASK = {}'.format(TRAIN_BATCH_SIZE,TASK))\n",
        "  print('***** Finished training at {} takes {} seconds *****'.format(datetime.datetime.now(),end-start))\n",
        "\n",
        "\n",
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "  eval_examples = processor.get_dev_examples(ESTIMATOR_DIR)\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  start = time.time()\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  #clear_output()\n",
        "  end = time.time()\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "  print('***** Finished evaluation at {} takes {} seconds *****'.format(datetime.datetime.now(),end-start))\n",
        "  timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "  output_eval_file = os.path.join(MODEL_OUTPUT_DIR, \"eval_results.\" + timestr + \".txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "      \n",
        "def model_predict(estimator, fileName):\n",
        "    use_tpu = True\n",
        "\n",
        "    start0 = time.time()\n",
        "\n",
        "    #predict_examples = processor.get_test_examples(DATA_DIR)\n",
        "    inputFile = os.path.join(INPUT_DATA_DIR, fileName + \".tsv\")\n",
        "    print('reading from {}'.format(inputFile))\n",
        "    predict_examples = processor._create_examples(processor._read_tsv(inputFile), \"test\")\n",
        "    num_actual_predict_examples = len(predict_examples)\n",
        "    if use_tpu:\n",
        "      # TPU requires a fixed batch size for all batches, therefore the number\n",
        "      # of examples must be a multiple of the batch size, or else examples\n",
        "      # will get dropped. So we pad with fake examples which are ignored\n",
        "      # later on.\n",
        "      while len(predict_examples) % PREDICT_BATCH_SIZE != 0:\n",
        "        predict_examples.append(run_classifier.PaddingInputExample())\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    predict_file = os.path.join(MODEL_OUTPUT_DIR, \"predict.tf_record.\" + fileName + '.' + timestr)\n",
        "    run_classifier.file_based_convert_examples_to_features(predict_examples, \n",
        "                                                           label_list,\n",
        "                                            MAX_SEQ_LENGTH, tokenizer,\n",
        "                                            predict_file)\n",
        "\n",
        "\n",
        "    start = time.time()\n",
        "    print('** Convert examples to features took {} seconds'.format(datetime.datetime.now(),start-start0))\n",
        "    print(\"***** Running prediction*****\")\n",
        "\n",
        "    predict_drop_remainder = True if use_tpu else False\n",
        "    predict_input_fn = run_classifier.file_based_input_fn_builder(\n",
        "        input_file=predict_file,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=predict_drop_remainder)\n",
        "\n",
        "    result = estimator.predict(input_fn=predict_input_fn)\n",
        "\n",
        "    output_predict_file = os.path.join(RESULT_DIR, \"predict_results.\" + fileName + \".tsv\")\n",
        "    output_predict_file1 = os.path.join(MODEL_OUTPUT_DIR, \"predict_results.\" + fileName + \".tsv\")\n",
        "    with tf.gfile.GFile(output_predict_file, \"w\") as writer, tf.gfile.GFile(output_predict_file1, \"w\") as writer1:\n",
        "      #clear_output()\n",
        "      num_written_lines = 0\n",
        "      print(\"***** Predict results *****\")\n",
        "      for (i, prediction) in enumerate(result):\n",
        "        probabilities = prediction[\"probabilities\"]\n",
        "        if i >= num_actual_predict_examples:\n",
        "          break\n",
        "        output_line = \"\\t\".join(\n",
        "            str(class_probability)\n",
        "            for class_probability in probabilities) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "        writer1.write(output_line)\n",
        "        num_written_lines += 1\n",
        "    assert num_written_lines == num_actual_predict_examples  \n",
        "    end = time.time()\n",
        "    print(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(predict_examples), num_actual_predict_examples,\n",
        "                    len(predict_examples) - num_actual_predict_examples)\n",
        "    print(\"  Batch size = %d\", PREDICT_BATCH_SIZE)\n",
        "    print('***** Finished testing at {} takes {} seconds *****'.format(datetime.datetime.now(),end-start))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmJijCO_LauS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    save_summary_steps = SAVE_SUMMARY_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(MODEL_OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}